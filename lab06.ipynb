{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646e18d3-2150-41f2-b915-08005081392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 22:37:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a5f5bf954455:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[28]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[28] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[28]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"32g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3944078-26e7-43f0-a043-2113c3cd2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-28 22:37:27--  https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip\n",
      "128.195.10.252ive.ics.uci.edu (archive.ics.uci.edu)... \n",
      "connected. to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘recipe+reviews+and+user+feedback+dataset.zip’\n",
      "\n",
      "recipe+reviews+and+     [        <=>         ]   2.02M   294KB/s    in 6.6s    \n",
      "\n",
      "2025-01-28 22:37:34 (314 KB/s) - ‘recipe+reviews+and+user+feedback+dataset.zip’ saved [2114088]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a7642c-5705-4297-b613-290dd7b3d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab05.ipynb\t\t       pan-tadeusz.txt\n",
      "lab06.ipynb\t\t       polish.stopwords.txt\n",
      "lab5.ipynb\t\t       recipe+reviews+and+user+feedback+dataset.zip\n",
      "pan_tadeusz_bag_of_words.json  venv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c523df3a-cd49-46fa-8d9b-0adc2243fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv recipe+reviews+and+user+feedback+dataset.zip recipe_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d264b676-4009-465c-a0c2-b18db8b6f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# wypakowujemy plik do podfolderu data\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"recipe_reviews.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce12fd4-1d97-4611-9c04-1cd9abefbb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Recipe Reviews and User Feedback Dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1725c502-b80f-412e-9abc-fbb6b03e4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",recipe_number,recipe_code,recipe_name,comment_id,user_id,user_name,user_reputation,created_at,reply_count,thumbs_up,thumbs_down,stars,best_score,text\n",
      "0,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2G3aneMRgRMZwXqIHmSdXSG1hEM,u_9iFLIhMa8QaG,Jeri326,1,1665619889,0,0,0,5,527,\"I tweaked it a little, removed onions because of onion haters in my house, used Italian seasoning instead of just oregano, and use a paprika/ cayenne mix and a little more than the recipe called for.. we like everything a bit more hot. The chili was amazing! It was easy to make and everyone absolutely loved it. It will now be a staple meal in our house.\"\n",
      "1,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2FsPC83HtzCsQAtOxlbL6RcaPbY,u_Lu6p25tmE77j,Mark467,50,1665277687,0,7,0,5,724,\"Bush used to have a white chili bean and it made this recipe super simple. I’ve written to them and asked them to please!, bring them back\"\n"
     ]
    }
   ],
   "source": [
    "# sprawdzamy jak wyglądają 3 pierwsze linie pliku, widać, że pierwsza zawiera nagłówki kolumn a dane są oddzielone przecinkiem\n",
    "!head -3 \"data/Recipe Reviews and User Feedback Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf745ec2-7e5b-4f1c-9126-7182edf69022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f286a7-88ff-42d0-b332-f01fcebbff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id| user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|   Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|   Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  3|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_fqrybAdYjgjG|jeansch123|              1|1661787808|          2|        2|          0|    0|       581|In your introduct...|\n",
      "|  4|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_XXWKwVhKZD69|  camper77|             10|1664913823|          1|        7|          0|    0|       820|Wonderful! I made...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:38:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# najpopularniejsza metoda ich pobrania to show(), ale jest ich więcej\n",
    "df_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdf96b4-3732-4035-aead-74a81993ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rzut oka na schemę tego DataFrame\n",
    "df_reviews.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c73eca3-169b-4c05-b162-b4c1206f462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widać, że wszystkie kolumny są typu string, to jest domyślny sposób wczytywania danych przez spark z plain text\n",
    "# możemy jednak przekazać dodatkowy parametr, który na podstawie próbki danych spróbuje dobrać typ danych odpowiedni dla kolumny\n",
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9de2ae-2e18-474e-a0c6-d694066efdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- thumbs_up: integer (nullable = true)\n",
      " |-- thumbs_down: integer (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- best_score: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# po wypisaniu schemy widać zmianę\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78a3f91-54c5-4a9f-8626-1f4429bc19a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ramkę możemy również inicjalizować wskazując pożądane typy danych\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, LongType\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",1000)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\", StringType(), True), \\\n",
    "    StructField(\"user_id\", StringType(), True), \\\n",
    "    StructField(\"lastname\", StringType(), True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    # błąd konwersji \"\" na int!\n",
    "    # StructField(\"id\", LongType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", StringType(), True)\n",
    "    # chcielibyśmy tak, ale tutaj nie da się za bardzo - błąd konwersji int na decimal!\n",
    "    # StructField(\"salary\", DecimalType(10,2), True) \\\n",
    "  ])\n",
    "\n",
    "df_test = spark.createDataFrame(data=data,schema=schema)\n",
    "df_test.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522c1d5f-ad65-4661-92f0-9aec91f48e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# możemy wykonać rzutowanie po wczytaniu danych z większością kolumn typu tekstowego\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_test = df_test.withColumn(\"salary\", F.col(\"salary\").cast(\"decimal(10,2)\"))\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6f5e8d-a345-49cd-8475-17e7e1a38a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| salary|\n",
      "+-------+\n",
      "|3000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|1000.00|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wyświetlenie danych z pojedynczej kolumny\n",
    "df_test.select(df_test.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ff8e13-f45d-4ab7-b9fc-28a6deebc1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18268"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ile wierszy w ramce?\n",
    "df_reviews.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444ef856-f868-4499-be34-1de897721669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'user_name'>, Column<'user_name'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame składa się z obiektów typu Column dla każdej kolumny\n",
    "# API dla typu Column: https://spark.apache.org/docs/3.5.3/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html\n",
    "\n",
    "# do kolumn możemy się odwoływać tak jak w pandas API, ale wynik jest inny\n",
    "df_reviews.user_name, df_reviews['user_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16aeebc-fe62-4464-a2a7-392653715558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| user_name|\n",
      "+----------+\n",
      "|   Jeri326|\n",
      "|   Mark467|\n",
      "|Barbara566|\n",
      "|jeansch123|\n",
      "|  camper77|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aby wyświetlić dane musimy wywoałać funkcję select na obiekcie dataframe\n",
    "\n",
    "df_reviews.select(df_reviews.user_name).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c923bf7-e116-4446-98c8-e51aeba04606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_name: string, user_reputation: string]\n",
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do funkcji select możemy przekazać wiele kolumn a wywołania podobnie jak dla RDD są leniwe\n",
    "print(df_reviews.select(df_reviews.user_name, df_reviews.user_reputation))\n",
    "# musimy więc wywołać funkcję, której wykonanie \"zmusi\" Sparka do wyliczenia jej wartości lub jawnie wywołać np. show\n",
    "df_reviews.select(df_reviews.user_name, df_reviews.user_reputation).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9552bf08-3a37-404b-b7ff-2778d4420460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# lub indeksując kolumny innym sposobem\n",
    "df_reviews.select(df_reviews['user_name'],df_reviews['user_reputation']).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52326e21-38c9-4163-9a03-02847980de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:40:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|           45|         64|         74|        77|     80|       83|             84|        86|         86|       86|         86|   86|        86|  86|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# policzymy teraz liczbę wartości NULL w każdej kolumnie\n",
    "df_reviews.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f6a81ef-655f-44a4-99cb-f959194d32df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|                 _c0|       recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|      Thank you!!!!\"|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| It was excellent!  |                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|The recipe was a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|A shoutout to the...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is de...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| like the time it...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|My one complaint ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|Have never eaten ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I thinned the mix...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|When you write 1-...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|decided to make a...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|looked good.  I u...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|little sweeter th...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I baked the crust...| just to crisp it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I made it myself ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I will try adding...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| big! I&#39;m wit...| WILL NOT MAKE it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|It says a small p...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|                  So| I used the compl...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:40:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# rzućmy okiem na kilka wierszy gdzie w kolumnie recipe_name jest wartość NULL\n",
    "df_reviews.filter(df_reviews.recipe_code.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "357c2127-8962-4574-ad91-f7230c3115a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:40:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18182"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zapisanie do nowej ramki danych bez wartości pustych\n",
    "df_reviews_clean = df_reviews.na.drop()\n",
    "df_reviews_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea2292cb-435f-4613-80f4-edcc87160996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:41:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|            0|          0|          0|         0|      0|        0|              0|         0|          0|        0|          0|    0|         0|   0|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dla pewności możemy to sprawdzić raz jeszcze\n",
    "df_reviews_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b9f842f-0c3b-47c1-b095-430086fbcdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     user_name|\n",
      "+--------------+\n",
      "|         ahmom|\n",
      "|  annamossburg|\n",
      "|   astarzynski|\n",
      "|     adamscook|\n",
      "|      angela32|\n",
      "|       annaf27|\n",
      "|    avanhaasen|\n",
      "|    aunt ann's|\n",
      "|     angelic0w|\n",
      "|a_n_g_e_l_0715|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id|       user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|         Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|         Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|      Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  5|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_BALTQJIvWtYr|         nikhita|              1|1661354351|          0|        3|          1|    5|       518|amazing! my boyfr...|\n",
      "|  6|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_HuJVXMzQqJoI|       Sandy1256|              1|1644088805|          0|       11|          0|    5|       833|Wow!!!  This reci...|\n",
      "|  8|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_xDTU4BqIVIc9|           Quest|              1|1643933124|          0|        6|          0|    5|       693|I absolutely love...|\n",
      "|  9|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_cDoX9ujcQEoc|     Susannah953|              1|1643237839|          0|        0|          0|    5|       404|I make this a lot...|\n",
      "| 10|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_a1kMC63ejmcn|jillanglemyer810|              1|1643127683|          0|        3|          0|    5|       706|Best and easiest ...|\n",
      "| 11|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_rLDRFmNx35zU|       Leslie126|              1|1642892566|          0|        2|          1|    5|       617|Best white chili ...|\n",
      "| 12|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_lPW6uyGJNSN0|       Cindy4876|              1|1642353692|          0|        1|          0|    5|       633|This recipe was e...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:41:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n",
      "25/01/28 22:41:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# filtrowanie danych z ramki\n",
    "df_reviews_clean.filter(df_reviews.user_name.startswith('a')).select(df_reviews_clean.user_name).show(10)\n",
    "df_reviews_clean.filter(df_reviews.stars == 5).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8be7063-e868-4d61-a5fa-380381b2b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    avg(thumbs_up)|\n",
      "+------------------+\n",
      "|1.0892641073589264|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:41:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# wyliczenie średniej wartości z kolumny\n",
    "df_reviews_clean.select(avg(df_reviews_clean.thumbs_up)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6144e41-a988-4e20-89b6-b02929ca98f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         thumbs_up|\n",
      "+-------+------------------+\n",
      "|  count|             18182|\n",
      "|   mean|1.0892641073589264|\n",
      "| stddev| 4.201003572820717|\n",
      "|    min|                 0|\n",
      "|    max|               106|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:41:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ale możemy się dowiedzieć tego i więcej w sposób podobny do tego z biblioteki pandas\n",
    "df_reviews_clean.select(df_reviews_clean.thumbs_up).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f34694f-a627-42fe-a849-abd647029a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|recipe_code|sum(thumbs_down)|\n",
      "+-----------+----------------+\n",
      "|       2832|             488|\n",
      "|       9739|             354|\n",
      "|      17826|             328|\n",
      "|      18345|             313|\n",
      "|      12003|             306|\n",
      "|       4383|             301|\n",
      "|      41095|             275|\n",
      "|       8202|             272|\n",
      "|       6504|             264|\n",
      "|       6086|             262|\n",
      "+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:41:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_reviews_clean.groupby('recipe_code').agg({'thumbs_down': 'sum'}).sort(desc('sum(thumbs_down)')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46d15705-4bfb-4ac4-8bfc-c15abfc945aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deklaracja zbiorów wartości dla poszczególnych kolumn przyszłego zbioru danych\n",
    "header = ['id', 'firstname', 'lastname', 'age', 'salary']\n",
    "firstnames = ['Adam', 'Katarzyna', 'Krzysztof', 'Marek', 'Aleksandra', 'Zbigniew', 'Wojciech', 'Mieczysław', 'Agata', 'Wisława']\n",
    "lastnames = ['Mieczykowski', 'Kowalski', 'Malinowski' , 'Szczaw', 'Glut', 'Barański', 'Brzęczyszczykiewicz', 'Wróblewski', 'Wlotka', 'Pysla']\n",
    "age = {'min': 18, 'max': 68}\n",
    "salary = {'min': 3200, 'max': 12500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db3033-4b0c-4f60-98a7-d630ba8882b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a8a4f6a-4f26-4daa-8c80-fadd025e0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_dataset(filename, n_rows=100, chunk_size=100000):\n",
    "    rows = []\n",
    "    rows.append(header)\n",
    "    mu = (salary['max'] + salary['min']) / 2\n",
    "    sigma = 1000\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as filehandler:\n",
    "        \n",
    "        for id in tqdm(range(1, n_rows + 1), total=n_rows, desc=\"Building dataset...\"):\n",
    "            row = [\n",
    "                f'{id}', \n",
    "                f'{random.choice(firstnames)}', \n",
    "                f'{random.choice(lastnames)}', \n",
    "                f\"{random.randint(age['min'], age['max'])}\",\n",
    "                f\"{round(float(random.normalvariate(mu=mu, sigma=sigma)), 2)}\"\n",
    "            ]\n",
    "            rows.append(row)\n",
    "            if id % chunk_size == 0:\n",
    "                filehandler.writelines([f\"{','.join(row)}\\n\" for row in rows])\n",
    "                rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cad3e80-ef45-4baa-8d0e-bdaaa1314753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset...: 100%|███████████████████| 20000000/20000000 [01:28<00:00, 224836.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# około 715MB zostanie zapisanych w pliku csv, dostosuj ilość rekordów do swoich potrzeb\n",
    "build_dataset('employee.csv', 20_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4083bb60-7313-42d7-a525-7bf5976e7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==========>                                             (5 + 23) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 ms, sys: 5.05 ms, total: 12.9 ms\n",
      "Wall time: 2.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# więcej magicznych metod w Jupyter Notebooku: https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# wczytanie pliku csv przez spark\n",
    "# df = spark.read.csv('employee.csv', header=True)\n",
    "df = spark.read.csv('employee.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5a0b21f-0502-4deb-9124-7f2d2f92145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a21106f-1bdd-46aa-b053-a076b1b0f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+----------+-------------------+---+-------+\n",
      "| id| firstname|           lastname|age| salary|\n",
      "+---+----------+-------------------+---+-------+\n",
      "|  1| Krzysztof|       Mieczykowski| 31|7997.69|\n",
      "|  2|  Zbigniew|         Malinowski| 26|7493.97|\n",
      "|  3| Krzysztof|             Wlotka| 19|9745.33|\n",
      "|  4|     Agata|Brzęczyszczykiewicz| 43|7968.02|\n",
      "|  5| Krzysztof|             Szczaw| 39|8192.67|\n",
      "|  6|   Wisława|               Glut| 35|8441.91|\n",
      "|  7|      Adam|Brzęczyszczykiewicz| 60|7023.85|\n",
      "|  8|  Zbigniew|              Pysla| 67|6176.82|\n",
      "|  9|Mieczysław|Brzęczyszczykiewicz| 48|7611.33|\n",
      "| 10|     Agata|       Mieczykowski| 26|8180.23|\n",
      "+---+----------+-------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 5.08 ms, sys: 1.05 ms, total: 6.13 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wypisujemy schemat i 10 pierwszych wierszy utworzonego obiektu Spark DataFrame\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "\n",
    "# przykład wykorzystania funkcji transform, która mapuje wykonanie stworzonej funkcji tu_upper_str_columns na istniejącą kolumnę\n",
    "# i zwraca nową ramkę z dodatkową kolumną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3532c24-aeda-4a49-88ac-68d81f796726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(df, column_name, new_column_name):\n",
    "    return df.withColumn(new_column_name, upper(df[column_name]))\n",
    "df = df.transform(to_upper_str_columns, \"firstname\", \"firstname_upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70989ef3-7fc3-47fb-a7a9-fccaac149a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+-------+---------------+\n",
      "| id| firstname|           lastname|age| salary|firstname_upper|\n",
      "+---+----------+-------------------+---+-------+---------------+\n",
      "|  1| Krzysztof|       Mieczykowski| 31|7997.69|      KRZYSZTOF|\n",
      "|  2|  Zbigniew|         Malinowski| 26|7493.97|       ZBIGNIEW|\n",
      "|  3| Krzysztof|             Wlotka| 19|9745.33|      KRZYSZTOF|\n",
      "|  4|     Agata|Brzęczyszczykiewicz| 43|7968.02|          AGATA|\n",
      "|  5| Krzysztof|             Szczaw| 39|8192.67|      KRZYSZTOF|\n",
      "|  6|   Wisława|               Glut| 35|8441.91|        WISŁAWA|\n",
      "|  7|      Adam|Brzęczyszczykiewicz| 60|7023.85|           ADAM|\n",
      "|  8|  Zbigniew|              Pysla| 67|6176.82|       ZBIGNIEW|\n",
      "|  9|Mieczysław|Brzęczyszczykiewicz| 48|7611.33|     MIECZYSŁAW|\n",
      "| 10|     Agata|       Mieczykowski| 26|8180.23|          AGATA|\n",
      "+---+----------+-------------------+---+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d19ad97-fa9c-43ec-94b5-75622185a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "315830"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtrowanie numeryczne, ale tu na kolumnie typu str - czy jest poprawne?\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e803b8b-90bf-4155-91e9-5ac4a6e834a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# na ile partycji została nasza ramka danych rozrzucona po \"klastrze\"?\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6bd6dd7-1589-4213-a4eb-c54198a0462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:==================>                                     (9 + 19) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.59 ms, sys: 3.1 ms, total: 10.7 ms\n",
      "Wall time: 1.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "315830"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy domyślnej liczbie partycji\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2937ad4-70b5-462e-ba92-6931f3b057f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "faca93d6-c478-4183-8726-428dba367519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:============================================>           (22 + 6) / 28]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd7d78-267a-449d-a237-1cc6ae8f1345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "241e7abf-d859-409b-b034-68e2ed30be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====================>                                 (11 + 17) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.83 ms, sys: 2.03 ms, total: 10.9 ms\n",
      "Wall time: 1.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "315830"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy 12 partycjach dla 20_000_000 rekordów\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e80f97ba-7238-4642-b5ce-3a9d8bdd44a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:45:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 81:>                                                       (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|firstname_age|   18|   19|   20|   21|   22|   23|   24|   25|   26|   27|   28|   29|   30|   31|   32|   33|   34|   35|   36|   37|   38|   39|   40|   41|   42|   43|   44|   45|   46|   47|   48|   49|   50|   51|   52|   53|   54|   55|   56|   57|   58|   59|   60|   61|   62|   63|   64|   65|   66|   67|   68|\n",
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|         Adam|38957|39229|39021|39017|39274|39561|39656|39354|39334|39095|38893|38997|38988|39275|39372|39209|38916|39324|39360|39239|39635|39382|39315|39365|39455|39135|38833|39156|39114|38960|39242|39184|39359|39227|39280|39530|39485|39519|39319|38746|39244|39387|39093|39408|39015|39356|38919|39537|39208|39251|39340|\n",
      "|        Agata|39588|39525|39010|38937|39200|39294|39417|39265|38956|39166|38868|39274|39188|39050|39233|39202|39181|38976|38964|39295|39218|39416|39158|39286|39470|39453|39170|39144|39294|39074|39331|39412|39118|39098|39602|39454|39473|39199|39573|39189|38982|39442|38985|39221|39378|39299|38716|39248|38968|39391|39154|\n",
      "|   Aleksandra|39117|39401|39496|39525|38860|38913|39329|39020|39507|39047|39102|39197|39255|38996|39349|39370|39552|38956|39075|39077|39417|39339|39308|39435|39345|39210|38984|39023|39163|39163|39404|39123|39097|39027|39347|39505|38732|39487|39059|39246|38932|39200|39211|38665|39420|39035|39248|39130|39240|39290|39233|\n",
      "|    Katarzyna|39457|39393|39384|38884|38893|39383|39270|39382|38688|39270|39285|39253|39068|39374|39262|39353|39052|38921|38855|39528|39217|39170|39037|39325|39291|38964|39478|39120|39292|39325|39139|39156|38918|39073|39362|39432|39010|39056|39387|39417|38959|38854|39223|39288|38777|39661|39089|39132|38921|39274|39304|\n",
      "|    Krzysztof|38775|38870|39238|39071|39082|39199|39548|39362|39389|39398|39180|39096|39100|39410|39315|38889|39082|39002|39485|39401|39543|39082|39337|39218|39237|39231|39375|39307|39104|39006|39257|39078|38996|39243|39288|39074|39101|39153|39372|39268|39077|39334|39362|39135|39217|39459|39191|39434|39189|39024|39191|\n",
      "|        Marek|39265|39554|38906|39194|39220|39197|39000|39262|39391|39088|39367|39522|39271|39414|39121|39088|39405|39104|39190|39233|39434|39442|39223|39294|39554|39291|39209|39399|39630|39297|39220|39121|39131|39294|39355|39233|39174|39263|39415|39238|39495|39211|39065|38792|39011|38805|39307|39205|39303|39248|39202|\n",
      "|   Mieczysław|38952|39625|39415|39033|39004|38566|38984|39496|39331|39286|39268|39446|39094|38985|39387|39340|39077|38995|39188|39390|39291|39215|39202|39388|39346|39224|39316|39127|39133|39105|39267|39294|39225|39127|38847|39748|39529|39205|39179|39042|39437|39272|39566|39101|39028|39100|39673|39608|39197|38980|39168|\n",
      "|      Wisława|39085|39594|39387|39171|39000|39093|39343|39179|38896|39127|39027|39216|39341|38964|39254|39091|38958|39102|39030|39128|39333|38713|39133|39534|38900|39043|39185|39069|39256|39302|39206|39214|39380|39242|39305|39325|39246|38975|39107|39237|39542|39061|39288|39464|39329|39321|39004|39123|38643|39000|39462|\n",
      "|     Wojciech|39415|39287|39149|39062|39397|39668|38827|39066|39401|39038|39148|39043|39045|39338|39206|38966|39376|38930|39073|38950|39275|39194|39106|39279|39553|39234|39026|39175|39240|39138|39271|39160|39226|39122|39466|38949|39079|39312|38992|39342|39098|39118|38969|39304|39415|39590|39038|39047|39374|39436|39194|\n",
      "|     Zbigniew|39113|39338|39342|39270|39185|39109|39453|39233|39164|39403|39495|38879|39044|39303|39399|39356|39076|39251|38924|39294|39111|39279|39630|39058|39102|39173|39177|39288|39271|39152|39498|39126|39295|38891|39478|39143|39454|39163|39088|39295|39315|39259|39346|39215|39466|38832|39212|39526|39381|39245|39292|\n",
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# macierz częstości dla dwóch kolumn - uwaga dla bardzo różnorodnych danych!\n",
    "df.crosstab(\"firstname\", \"age\").sort(\"firstname_age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3f9d225-f9cc-4bd3-911c-766e4b83124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- Exchange (4)\n",
      "   +- Project (3)\n",
      "      +- Filter (2)\n",
      "         +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [5]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/spark/work-dir/employee.csv]\n",
      "PushedFilters: [IsNotNull(firstname), StringContains(firstname,ski)]\n",
      "ReadSchema: struct<id:int,firstname:string,lastname:string,age:int,salary:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [5]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538]\n",
      "Condition : (isnotnull(firstname#1535) AND Contains(firstname#1535, ski))\n",
      "\n",
      "(3) Project\n",
      "Output [6]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538, upper(firstname#1535) AS firstname_upper#1571]\n",
      "Input [5]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538]\n",
      "\n",
      "(4) Exchange\n",
      "Input [6]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538, firstname_upper#1571]\n",
      "Arguments: RoundRobinPartitioning(12), REPARTITION_BY_NUM, [plan_id=1316]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [6]: [id#1534, firstname#1535, lastname#1536, age#1537, salary#1538, firstname_upper#1571]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# funkcja explain może przydać się w przypadku bardziej zaawansowanego debuggingu, optymalizacji i zrozumienia\n",
    "# kolejności działania niektórych elementów silnika Spark\n",
    "query = df.filter(df.firstname.contains('ski'))\n",
    "query.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e216d706-a827-4d22-bf6b-0bbb2373ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zapisujemy ramkę do plików parquet\n",
    "# zwróć uwagę na liczbę utworzonych plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "783ab8af-92bd-4428-b694-65c4ce151ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:45:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/01/28 22:45:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/28 22:45:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/28 22:45:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/28 22:45:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/01/28 22:45:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/28 22:45:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/28 22:45:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/28 22:45:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.parquet('./data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18c58454-0e7a-4f52-a2c1-8b2f0d1bad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:45:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/01/28 22:45:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/28 22:45:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/28 22:45:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/28 22:45:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/01/28 22:45:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/01/28 22:45:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/01/28 22:45:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/01/28 22:45:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# lub chcąc nadpisać już istniejące dane - w trybie overwrite\n",
    "df.write.mode(\"overwrite\").parquet('./data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6e8d674-190a-41ab-bd4d-7aeca92810be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:47:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "| id|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id| user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|   Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|   Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  3|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_fqrybAdYjgjG|jeansch123|              1|1661787808|          2|        2|          0|    0|       581|In your introduct...|\n",
      "|  4|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_XXWKwVhKZD69|  camper77|             10|1664913823|          1|        7|          0|    0|       820|Wonderful! I made...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 22:47:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicjalizacja SparkSession\n",
    "spark = SparkSession.builder.appName(\"Recipe Reviews\").getOrCreate()\n",
    "\n",
    "# Wczytanie danych\n",
    "data = spark.read.csv(\"data/Recipe Reviews and User Feedback Dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Zmień nazwę kolumny\n",
    "data = data.withColumnRenamed(\"_c0\", \"id\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1756c006-e859-4693-86ec-07f9c9b9073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|reply_count|\n",
      "+-----------+\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"reply_count\") \\\n",
    "    .orderBy(\"reply_count\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ed2d6f2-c838-4349-b109-a405dc679ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|         recipe_name|total_best_score|\n",
      "+--------------------+----------------+\n",
      "|   Cheeseburger Soup|           98863|\n",
      "|  Creamy White Chili|           85497|\n",
      "|Amish Breakfast C...|           64880|\n",
      "|Best Ever Banana ...|           64247|\n",
      "|Favorite Chicken ...|           60755|\n",
      "|Basic Homemade Bread|           59867|\n",
      "|Flavorful Chicken...|           59195|\n",
      "|Zucchini Pizza Ca...|           54032|\n",
      "|Enchilada Casser-...|           51975|\n",
      "|    Cauliflower Soup|           47905|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "data.groupBy(\"recipe_name\") \\\n",
    "    .agg(sum(\"best_score\").alias(\"total_best_score\")) \\\n",
    "    .orderBy(\"total_best_score\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "192c9e16-5bd5-4ff7-90e6-f854a301a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|         recipe_name|total_comments|\n",
      "+--------------------+--------------+\n",
      "|   Cheeseburger Soup|           725|\n",
      "|  Creamy White Chili|           654|\n",
      "|Best Ever Banana ...|           509|\n",
      "|Enchilada Casser-...|           421|\n",
      "|Basic Homemade Bread|           397|\n",
      "|Favorite Chicken ...|           395|\n",
      "|Flavorful Chicken...|           368|\n",
      "|Amish Breakfast C...|           338|\n",
      "|Zucchini Pizza Ca...|           332|\n",
      "|    Cauliflower Soup|           324|\n",
      "+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "data.groupBy(\"recipe_name\") \\\n",
    "    .agg(count(\"reply_count\").alias(\"total_comments\")) \\\n",
    "    .orderBy(\"total_comments\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2786d53-e347-4c76-85ba-48737c333d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|stars|count|\n",
      "+-----+-----+\n",
      "| NULL|   86|\n",
      "|    0| 1696|\n",
      "|    1|  280|\n",
      "|    2|  232|\n",
      "|    3|  490|\n",
      "|    4| 1655|\n",
      "|    5|13829|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"stars\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"stars\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4468cca-6cf9-4be2-8e7a-3d0b68366319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:================================>                     (17 + 11) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+---------+-------------------+---+-------+\n",
      "| id|firstname|           lastname|age| salary|\n",
      "+---+---------+-------------------+---+-------+\n",
      "|  1|Krzysztof|       Mieczykowski| 31|7997.69|\n",
      "|  2| Zbigniew|         Malinowski| 26|7493.97|\n",
      "|  3|Krzysztof|             Wlotka| 19|9745.33|\n",
      "|  4|    Agata|Brzęczyszczykiewicz| 43|7968.02|\n",
      "|  5|Krzysztof|             Szczaw| 39|8192.67|\n",
      "+---+---------+-------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_data = spark.read.csv(\"employee.csv\", header=True, inferSchema=True)\n",
    "employee_data.printSchema()\n",
    "employee_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bee350f1-9a6e-4d9c-a4cf-61beefda3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:=============================================>         (23 + 5) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas wykonania operacji z typem string: 1.89 sekundy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "string_employee_data = employee_data.withColumn(\"salary\", employee_data[\"salary\"].cast(\"string\"))\n",
    "start_time = time.time()\n",
    "\n",
    "count = string_employee_data.filter(string_employee_data[\"salary\"].cast(\"int\") > 10000).count()\n",
    "\n",
    "print(f\"Czas wykonania operacji z typem string: {time.time() - start_time:.2f} sekundy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ce19538-2296-42e5-8705-1383a53f0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:===================================================>   (26 + 2) / 28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas wykonania operacji z typem numeric: 1.48 sekundy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "count = employee_data.filter(employee_data[\"salary\"] > 10000).count()\n",
    "\n",
    "print(f\"Czas wykonania operacji z typem numeric: {time.time() - start_time:.2f} sekundy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a195acae-a720-45dd-a9c4-bdeac4acc3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f3750b3-1f73-4bcb-9a2d-54a0d8f187db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|age|age_bucket|\n",
      "+---+----------+\n",
      "| 31|       2.0|\n",
      "| 26|       1.0|\n",
      "| 19|       0.0|\n",
      "| 43|       3.0|\n",
      "| 39|       2.0|\n",
      "| 35|       2.0|\n",
      "| 60|       5.0|\n",
      "| 67|       5.0|\n",
      "| 48|       3.0|\n",
      "| 26|       1.0|\n",
      "| 23|       1.0|\n",
      "| 33|       2.0|\n",
      "| 51|       4.0|\n",
      "| 47|       3.0|\n",
      "| 65|       5.0|\n",
      "| 64|       5.0|\n",
      "| 21|       1.0|\n",
      "| 48|       3.0|\n",
      "| 21|       1.0|\n",
      "| 50|       4.0|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=splits,\n",
    "    inputCol=\"age\",\n",
    "    outputCol=\"age_bucket\"\n",
    ")\n",
    "\n",
    "# Podziel dane na buckety\n",
    "bucketed_data = bucketizer.transform(employee_data)\n",
    "\n",
    "bucketed_data.select(\"age\", \"age_bucket\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77333670-6ffa-4b7d-b5ba-3b69c7c50934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
